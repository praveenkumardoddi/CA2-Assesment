# -*- coding: utf-8 -*-
"""Sandwip Modak-0002_20231218173441_AWC-Fwd PN  MSC in Data Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VzKYx1hc9xwDrcie2-ipXGw6HYIoNhUX
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

file_path = 'THA21.20231218052819.csv'
data = pd.read_csv(file_path)

# Display the first few rows of the dataset
print(data.head())

print(data.tail())

print(data.describe())

print(data.info())

data.isnull()

data.isnull().sum()

# Visualization 1: Line plot - Weekly volume of cars
plt.figure(figsize=(10, 6))
sns.lineplot(x='Weeks of the year', y='VALUE', data=data)
plt.title('Weekly Volume of Cars')
plt.xlabel('Weeks of the year')
plt.ylabel('Volume')
plt.show()

# Visualization 2: Bar plot - Yearly comparison of traffic volume
plt.figure(figsize=(8, 5))
sns.barplot(x='Year', y='VALUE', data=data)
plt.title('Yearly Comparison of Traffic Volume')
plt.xlabel('Year')
plt.ylabel('Volume')
plt.show()

# Visualization 3: Box plot - Distribution of traffic volume
plt.figure(figsize=(8, 5))
sns.boxplot(y='VALUE', data=data)
plt.title('Distribution of Traffic Volume')
plt.ylabel('Volume')
plt.show()

# Visualization 4: Violin plot - Distribution of traffic volume by year
plt.figure(figsize=(8, 5))
sns.violinplot(x='Year', y='VALUE', data=data)
plt.title('Distribution of Traffic Volume by Year')
plt.xlabel('Year')
plt.ylabel('Volume')
plt.show()

# Visualization 5: Pairplot - Pairwise relationships between numerical variables
sns.pairplot(data)
plt.show()

# Visualization 6: Histogram - Distribution of weekly volume
plt.figure(figsize=(8, 5))
plt.hist(data['VALUE'], bins=20, alpha=0.7)
plt.title('Distribution of Weekly Volume')
plt.xlabel('Volume')
plt.ylabel('Frequency')
plt.show()

# Visualization 7: Heatmap - Correlation between numerical variables
plt.figure(figsize=(8, 6))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# Visualization 8: Swarm plot - Distribution of traffic volume by statistic label
plt.figure(figsize=(10, 6))
sns.swarmplot(x='Statistic Label', y='VALUE', data=data)
plt.title('Distribution of Traffic Volume by Statistic Label')
plt.xlabel('Statistic Label')
plt.ylabel('Volume')
plt.xticks(rotation=45)
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

data = pd.read_csv('THA21.20231218052819.csv')

# Assuming 'VALUE' is the target feature (weekly volume of cars) and other relevant features are used for prediction
features = ['Year', 'C03910V04662', 'C01198V01436']  # Adjust as needed
target = 'VALUE'

data = data[[target] + features]

data = data.dropna()

X = data.drop(target, axis=1)
y = data[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor()

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Use GridSearchCV to search for best hyperparameters
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

predictions = best_model.predict(X_test)
mse = mean_squared_error(y_test, predictions)
print(f"Best Parameters: {best_params}")
print(f"Mean Squared Error on Test Set: {mse}")

from statsmodels.tsa.statespace.sarimax import SARIMAX
import matplotlib.pyplot as plt

data = data[['VALUE', 'Year', 'C03910V04662', 'C01198V01436']]

data['Year'] = pd.to_datetime(data['Year'], format='%Y')

# Set 'Year' as the index
data.set_index('Year', inplace=True)

# Resampling to weekly frequency
data = data.resample('W').mean()

data = data.dropna()

# Splitting the data into training and testing sets
train_size = int(len(data) * 0.8)
train_data, test_data = data.iloc[:train_size], data.iloc[train_size:]

# Fit SARIMA model
order = (1, 1, 1)  # (p, d, q)
seasonal_order = (1, 1, 1, 12)  # (P, D, Q, S)
model = SARIMAX(train_data['VALUE'], order=order, seasonal_order=seasonal_order)
result = model.fit()

# Forecast
forecast = result.get_forecast(steps=len(test_data))
forecast_values = forecast.predicted_mean
confidence_intervals = forecast.conf_int()

# Plotting
plt.figure(figsize=(12, 6))
plt.plot(train_data.index, train_data['VALUE'], label='Training Data')
plt.plot(test_data.index, test_data['VALUE'], label='Actual Data')
plt.plot(test_data.index, forecast_values, label='Forecast')
plt.fill_between(test_data.index, confidence_intervals.iloc[:, 0], confidence_intervals.iloc[:, 1], color='gray', alpha=0.3)
plt.legend()
plt.title('Time Series Forecasting with SARIMA')
plt.xlabel('Year')
plt.ylabel('Volume of Cars')
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

data = pd.read_csv('THA21.20231218052819.csv')

features = ['Year', 'C03910V04662', 'C01198V01436']  # Adjust as needed

data = data[features]

data = data.dropna()

# Normalize the data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

data_scaled

# Choosing the number of clusters using the Elbow Method
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_scaled)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 6))
plt.plot(range(1, 11), inertia, marker='o', linestyle='--')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.show()

optimal_k = 3  # Adjust as needed

kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans.fit_predict(data_scaled)

data['Cluster'] = clusters

# Visualizing the clusters (considering two features at a time for plotting)
for i in range(len(features) - 1):
    for j in range(i + 1, len(features)):
        plt.figure(figsize=(8, 6))
        plt.scatter(data.iloc[:, i], data.iloc[:, j], c=clusters, cmap='viridis')
        plt.xlabel(features[i])
        plt.ylabel(features[j])
        plt.title(f'Clustering based on {features[i]} and {features[j]}')
        plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

import nltk
nltk.download('punkt', download_dir='path/to/nltk_data')
nltk.download('wordnet', download_dir='path/to/nltk_data')
nltk.download('stopwords', download_dir='path/to/nltk_data')

data = pd.read_csv('THA21.20231218052819.csv')

text_data = data['Transport Traffic Site']

sentiment_labels = data['Statistic Label']

def preprocess_text(text):
    words = word_tokenize(text.lower())  # Tokenization and convert to lowercase
    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha()]  # Lemmatization
    words = [word for word in words if word not in stop_words]  # Remove stopwords
    return ' '.join(words)

text_data = text_data.apply(preprocess_text)

# Feature Engineering
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(text_data)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, sentiment_labels, test_size=0.2, random_state=42)

# Train a classifier (Support Vector Machine for example)
clf = SVC(kernel='linear')
clf.fit(X_train, y_train)

# Predict sentiment on test set
predictions = clf.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, predictions)
print(f"Accuracy: {accuracy}")

pip install wordcloud

pip install pandas matplotlib wordcloud

import pandas as pd
import matplotlib.pyplot as plt

url = 'THA21.20231218052819.csv'
df = pd.read_csv(url)

df['Week Number'] = df['Weeks of the year'].str.extract('(\d+)').astype(int)

selected_statistic = 'THA21C01'
filtered_data = df[df['STATISTIC'] == selected_statistic]

data = pd.read_csv('TOA02.20231222105456.csv')

print(data.head())

print(data.describe())

print(data.info())

data.isnull()

data.isnull().sum()

data.replace('-', pd.NA, inplace=True)

data['VALUE'] = pd.to_numeric(data['VALUE'], errors='coerce')

plt.figure(figsize=(8, 6))
sns.barplot(x='Days of Week', y='VALUE', data=data)
plt.title('Average Flow per Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Average Flow')
plt.show()

plt.figure(figsize=(10, 6))
sns.lineplot(x='Year', y='VALUE', data=data)
plt.title('Flow Variation Over the Year')
plt.xlabel('Year')
plt.ylabel('Flow')
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(data['VALUE'], bins=20, kde=True)
plt.title('Histogram of Flow Values')
plt.xlabel('Flow')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(x='Days of Week', data=data)
plt.title('Number of Observations on Each Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Count')
plt.show()

from scipy.stats import ttest_1samp


hypothesized_mean = 500
t_statistic, p_value = ttest_1samp(data['VALUE'].dropna(), hypothesized_mean)

print(f"One-Sample T-Test Results:")
print(f"T-statistic: {t_statistic}")
print(f"P-value: {p_value}")

from scipy.stats import ttest_ind


t_statistic, p_value = ttest_ind(data['VALUE'].dropna(), data['VALUE'].dropna())

print(f"Two-Sample T-Test Results:")

print(f"P-value: {p_value}")

from scipy.stats import pearsonr

# Assuming you want to test the correlation between 'Year' and 'VALUE'
correlation_coefficient, p_value = pearsonr(data['Year'].dropna(), data['VALUE'].dropna())

print(f"Pearson's Correlation Test Results:")
print(f"Correlation Coefficient: {correlation_coefficient}")
print(f"P-value: {p_value}")

from scipy.stats import wilcoxon

# Considering 'VALUE' for two different days (e.g., Sunday and Monday) for the test
sunday_values = data[data['Days of Week'] == 'Sunday']['VALUE']
monday_values = data[data['Days of Week'] == 'Monday']['VALUE']

# Performing the Wilcoxon signed-rank test
statistic, p_value = wilcoxon(sunday_values, monday_values)

# Output the results
print(f"Wilcoxon Statistic: {statistic}")
print(f"P-value: {p_value}")

# Interpret the p-value
alpha = 0.05  # significance level
if p_value < alpha:
    print("Reject the null hypothesis: There is a significant difference between Sunday and Monday values.")
else:
    print("Cannot reject the null hypothesis: No significant difference between Sunday and Monday values.")

from scipy.stats import chi2_contingency

# Contingency table between 'Days of Week' and 'STATISTIC'
contingency_table = pd.crosstab(data['Days of Week'], data['STATISTIC'])

# Perform the chi-squared test
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

# Output the results
print(f"Chi-Squared Statistic: {chi2_stat}")
print(f"P-value: {p_val}")
print(f"Degrees of Freedom: {dof}")
print("Expected Frequencies:")
print(expected)

# Interpret the p-value
alpha = 0.05  # significance level
if p_val < alpha:
    print("Reject the null hypothesis: There is a significant association between 'Days of Week' and 'STATISTIC'.")
else:
    print("Cannot reject the null hypothesis: No significant association between 'Days of Week' and 'STATISTIC'.")

from scipy.stats import ttest_ind

# Extract 'VALUE' for Sundays and Mondays
sunday_values = data[data['Days of Week'] == 'Sunday']['VALUE']
monday_values = data[data['Days of Week'] == 'Monday']['VALUE']

# Perform independent two-sample t-test
t_stat, p_value = ttest_ind(sunday_values, monday_values, equal_var=False)

# Output the results
print(f"T-statistic: {t_stat}")
print(f"P-value: {p_value}")

# Interpretation
alpha = 0.05  # significance level
if p_value < alpha:
    print("Reject the null hypothesis: Significant difference in mean 'VALUE' between Sundays and Mondays.")
else:
    print("Cannot reject the null hypothesis: No significant difference in mean 'VALUE' between Sundays and Mondays.")

from scipy.stats import chi2_contingency

# Create a contingency table between 'Days of Week' and 'STATISTIC'
contingency_table = pd.crosstab(data['Days of Week'], data['STATISTIC'])

# Perform the chi-squared test
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

# Output the results
print(f"Chi-Squared Statistic: {chi2_stat}")
print(f"P-value: {p_val}")
print(f"Degrees of Freedom: {dof}")
print("Expected Frequencies:")
print(expected)

# Interpretation
alpha = 0.05  # significance level
if p_val < alpha:
    print("Reject the null hypothesis: Significant association between 'Days of Week' and 'STATISTIC'.")
else:
    print("Cannot reject the null hypothesis: No significant association between 'Days of Week' and 'STATISTIC'.")

